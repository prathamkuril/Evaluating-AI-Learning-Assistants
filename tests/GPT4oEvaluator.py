# Copyright (c) 2024 Braid Technologies Ltd

from openai import AzureOpenAI
from common.ApiConfiguration import ApiConfiguration
from BoxerDataTest_v5 import call_openai_chat
# from common.common_functions import call_openai_chat

MAX_RETRIES = 15                    # Maximum number of retries for API calls
NUM_QUESTIONS = 100   

# Function to call the OpenAI API with retry logic
@retry(wait=wait_random_exponential(min=5, max=15), stop=stop_after_attempt(MAX_RETRIES), retry=retry_if_not_exception_type(BadRequestError))
def call_openai_chat(chat_client: AzureOpenAI, messages: list[dict[str, str]], config: ApiConfiguration, logger: logging.Logger) -> str:
    """
    Retries the OpenAI chat API call with exponential backoff and retry logic.

    :param chat_client: An instance of the AzureOpenAI class.
    :type chat_client: AzureOpenAI
    :param messages: A list of dictionaries representing the messages to be sent to the API.
    :type messages: List[Dict[str, str]]
    :param config: An instance of the ApiConfiguration class.
    :type config: ApiConfiguration
    :param logger: An instance of the logging.Logger class.
    :type logger: logging.Logger
    :return: The content of the first choice in the API response.
    :rtype: str
    :raises RuntimeError: If the finish reason in the API response is not 'stop', 'length', or an empty string.
    :raises OpenAIError: If there is an error with the OpenAI API.
    :raises APIConnectionError: If there is an error with the API connection.
    """
    try:
        response = chat_client.chat.completions.create(
            model=config.azureDeploymentName,
            messages=messages,
            temperature=0.7,
            max_tokens=config.maxTokens,
            top_p=0.0,
            frequency_penalty=0,
            presence_penalty=0,
            timeout=config.openAiRequestTimeout,
        )
        content = response.choices[0].message.content
        finish_reason = response.choices[0].finish_reason

        if finish_reason not in {"stop", "length", ""}:
            logger.warning("Unexpected stop reason: %s", finish_reason)
            logger.warning("Content: %s", content)
            logger.warning("Consider increasing max tokens and retrying.")
            raise RuntimeError("Unexpected finish reason in API response.")

        return content

    except (OpenAIError, APIConnectionError) as e:
        logger.error(f"Error: {e}")
        raise


class GPT4oEvaluator:
    def __init__(self, config: ApiConfiguration):
        """
        Initialize the GPT4oEvaluator class.
        
        This class provides functions for evaluating the quality of summaries generated by a large language model (LLM)
        using GPT-4o via Azure OpenAI.
        
        Args:
            config (ApiConfiguration): Configuration object for Azure OpenAI.
        """
        self.config = config
        self.client = AzureOpenAI(
            azure_endpoint=config.resourceChatCompletionEndpoint,
            api_key=config.apiKey,
            api_version=config.apiVersion
        )

        self.system_instruction_prompt_eval = """Prompt: 
        You are a professional LLM evaluation judge assessing the quality of summaries generated by a large language model (LLM). 
        Your primary task is to evaluate how effectively the summary captures the core information from the original content and addresses the user's query.

        *** Instructions ***
        As a summary evaluator, follow these steps:

        - Understand the Question: Carefully read the original content to understand the key points and context.
        - Assess the Summary: Review the summary generated by the LLM, checking for its relevance, coherence, and completeness.
        - Rate the Summary: 
        1: Irrelevant or incoherent.
        2: Partially relevant but incomplete.
        3: Mostly relevant and coherent.
        4: Fully relevant and coherent.

        *** Response Format ***
        Return just the score as an integer (1, 2, 3, or 4).
        """

    def evaluate(self, original_content: str, summary: str) -> str:
        """
        Evaluates the quality of a summary based on the original content using GPT-4o.
        
        Args:
            original_content (str): The original text content that needs to be summarized.
            summary (str): The summary generated by the LLM that needs to be evaluated.
        
        Returns:
            str: The evaluation score as an integer value (1-4), assessing the summary's quality.
        """
        evaluation_prompt = f"""
        Question: {original_content}
        Summary: {summary}
        """

        messages = [
            {"role": "system", "content": self.system_instruction_prompt_eval},
            {"role": "user", "content": evaluation_prompt}
        ]

        response = call_openai_chat(self.client, messages, self.config, None)
        return response.strip()