# Copyright (c) 2024 Braid Technologies Ltd

"""
Question Generation (Persona):
Keeps using `gpt-4o` for persona-based question generation.
    
Similarity Embedding:
Upgraded to `text-embedding-3-large` for improved embedding precision and similarity calculations.
    
Evaluation LLM:
Continues with `gemini-1.5-pro` for evaluating the responses generated by GPT-4o.
"""

# Standard Library Imports
import logging
import os
import json
import sys
from logging import Logger
from typing import List, Dict, Any
import numpy as np
from numpy.linalg import norm
import datetime
import csv

# Third-Party Packages
from openai import AzureOpenAI, OpenAIError, BadRequestError, APIConnectionError
from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type
import google.generativeai as genai
from GeminiEvaluator import GeminiEvaluator
from GPT4oEvaluator import GPT4oEvaluator

# Add the project root and scripts directory to the Python path
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

# Local Modules
from common.ApiConfiguration import ApiConfiguration
from common.common_functions import get_embedding
from PersonaStrategy import DeveloperPersonaStrategy, TesterPersonaStrategy, BusinessAnalystPersonaStrategy, PersonaStrategy

# Constants
SIMILARITY_THRESHOLD = 0.8
MAX_RETRIES = 15
NUM_QUESTIONS = 100

# OpenAI prompts
OPENAI_PERSONA_PROMPT = "You are an AI assistant helping an application developer understand generative AI. You explain complex concepts in simple language, using Python examples if it helps. You limit replies to 50 words or less. If you don't know the answer, say 'I don't know'. If the question is not related to building AI applications, Python, or Large Language Models (LLMs), say 'That doesn't seem to be about AI'."
ENRICHMENT_PROMPT = "You will be provided with a question about building applications that use generative AI technology. Write a 50 word summary of an article that would be a great answer to the question. Consider enriching the question with additional topics that the question asker might want to understand. Write the summary in the present tense, as though the article exists. If the question is not related to building AI applications, Python, or Large Language Models (LLMs), say 'That doesn't seem to be about AI'.\n"
FOLLOW_UP_PROMPT = "You will be provided with a summary of an article about building applications that use generative AI technology. Write a question of no more than 10 words that a reader might ask as a follow up to reading the article."
FOLLOW_UP_ON_TOPIC_PROMPT = "You are an AI assistant helping a team of developers understand AI. You explain complex concepts in simple language. Respond 'yes' if the follow-up question is about AI, otherwise respond 'no'."

# Setup Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

gemini_evaluator = GeminiEvaluator()

def configure_openai_for_azure(config: ApiConfiguration, task: str) -> AzureOpenAI:
    if task == "chat":
        return AzureOpenAI(
            azure_endpoint=config.resourceChatCompletionEndpoint,
            api_key=config.apiKey.strip(),
            api_version=config.apiVersion
        )
    elif task == "embedding":
        return AzureOpenAI(
            azure_endpoint=config.resourceEmbeddingEndpoint,
            api_key=config.apiKey.strip(),
            api_version=config.apiVersion
        )

class TestResult:
    def __init__(self) -> None:
        self.question: str = ""
        self.hit_relevance: float = 0.0
        self.enriched_question_summary: str = ""
        self.hit: bool = False
        self.hit_relevance: float = 0.0
        self.follow_up: str = ""
        self.follow_up_on_topic: str = ""
        self.gemini_evaluation: str = ""
        self.gpt4o_evaluation: str = ""

@retry(wait=wait_random_exponential(min=5, max=15), stop=stop_after_attempt(MAX_RETRIES), retry=retry_if_not_exception_type(BadRequestError))
def call_openai_chat(chat_client: AzureOpenAI, messages: List[Dict[str, str]], config: ApiConfiguration, logger: logging.Logger) -> str:
    try:
        response = chat_client.chat.completions.create(
            model=config.azureDeploymentName,
            messages=messages,
            temperature=0.7,
            max_tokens=config.maxTokens,
            top_p=0.0,
            frequency_penalty=0,
            presence_penalty=0,
            timeout=config.openAiRequestTimeout,
        )
        content = response.choices[0].message.content
        finish_reason = response.choices[0].finish_reason

        if finish_reason not in {"stop", "length", ""}:
            logger.warning("Unexpected stop reason: %s", finish_reason)
            logger.warning("Content: %s", content)
            logger.warning("Consider increasing max tokens and retrying.")
            raise RuntimeError("Unexpected finish reason in API response.")

        return content

    except (OpenAIError, APIConnectionError) as e:
        logger.error(f"Error: {e}")
        raise

@retry(wait=wait_random_exponential(min=5, max=15), stop=stop_after_attempt(MAX_RETRIES), retry=retry_if_not_exception_type(BadRequestError))
def get_text_embedding(embedding_client: AzureOpenAI, config: ApiConfiguration, text: str, logger: Logger) -> np.ndarray:
    try:
        embedding = get_embedding(text, embedding_client, config)
        return np.array(embedding)
    except OpenAIError as e:
        logger.error(f"Error getting text embedding: {e}")
        raise

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    try:
        a, b = np.array(a), np.array(b)
    except Exception:
        raise ValueError("Input vectors must be numpy arrays or convertible to numpy arrays")

    if a.shape != b.shape:
        raise ValueError("Input vectors must have the same shape")

    dot_product = np.dot(a, b)
    a_norm, b_norm = norm(a), norm(b)

    if a_norm == 0 or b_norm == 0:
        raise ValueError("Input vectors must not be zero vectors")

    return dot_product / (a_norm * b_norm)

def generate_enriched_question(chat_client: AzureOpenAI, config: ApiConfiguration, question: str, logger: logging.Logger, llm_choice: str) -> str:
    if llm_choice == '1':  # Azure OpenAI
        messages = [
            {"role": "system", "content": OPENAI_PERSONA_PROMPT},
            {"role": "user", "content": ENRICHMENT_PROMPT + "Question: " + question},
        ]
        logger.info("Making API request to Azure OpenAI...")
        logger.info("Request payload: %s", messages)
        response = call_openai_chat(chat_client, messages, config, logger)
        logger.info("API response received: %s", response)
        return response
    elif llm_choice == '2':  # Gemini 1.5 Pro
        model = genai.GenerativeModel("gemini-1.5-pro")
        prompt = f"{OPENAI_PERSONA_PROMPT}\n\n{ENRICHMENT_PROMPT}\nQuestion: {question}"
        logger.info("Making API request to Gemini 1.5 Pro...")
        logger.info("Request payload: %s", prompt)
        response = model.generate_content(prompt)
        logger.info("API response received: %s", response.text)
        return response.text
    else:
        raise ValueError("Invalid LLM choice")

def generate_follow_up_question(chat_client: AzureOpenAI, config: ApiConfiguration, text: str, logger: logging.Logger) -> str:
    messages = [
        {"role": "system", "content": FOLLOW_UP_PROMPT},
        {"role": "user", "content": text},
    ]
    response = call_openai_chat(chat_client, messages, config, logger)
    return response

def assess_follow_up_on_topic(chat_client: AzureOpenAI, config: ApiConfiguration, follow_up: str, logger: logging.Logger) -> str:
    messages = [
        {"role": "system", "content": FOLLOW_UP_ON_TOPIC_PROMPT},
        {"role": "user", "content": follow_up},
    ]
    response = call_openai_chat(chat_client, messages, config, logger)
    return response

def process_questions(chat_client: AzureOpenAI, embedding_client: AzureOpenAI, config: ApiConfiguration, questions: List[str], processed_question_chunks: List[Dict[str, Any]], logger: logging.Logger, llm_choice: str) -> List[TestResult]:
    question_results: List[TestResult] = []
    for question in questions:
        question_result = TestResult()
        question_result.question = question
        question_result.enriched_question_summary = generate_enriched_question(chat_client, config, question, logger, llm_choice)
        embedding = get_text_embedding(embedding_client, config, question_result.enriched_question_summary, logger)

        best_hit_relevance = 0
        best_hit_summary = None

        for chunk in processed_question_chunks:
            if chunk and isinstance(chunk, dict):
                gpt4_embedding = chunk.get("embedding")
                similarity = cosine_similarity(gpt4_embedding, embedding)

                if similarity > SIMILARITY_THRESHOLD:
                    question_result.hit = True

                if similarity > best_hit_relevance:
                    best_hit_relevance = similarity
                    best_hit_summary = chunk.get("summary")
        
        question_result.hit_relevance = best_hit_relevance
        question_result.hit_summary = best_hit_summary

        if question_result.hit_summary:
            question_result.follow_up = generate_follow_up_question(chat_client, config, question_result.hit_summary, logger)
            question_result.follow_up_on_topic = assess_follow_up_on_topic(chat_client, config, question_result.follow_up, logger)
        
        if llm_choice == '1':
            question_result.gpt4o_evaluation = GPT4oEvaluator.evaluate(
                question_result.question,
                question_result.enriched_question_summary
            )
        elif llm_choice == '2':
            question_result.gemini_evaluation = GeminiEvaluator.evaluate(
                question_result.question,
                question_result.enriched_question_summary,
            )

        question_results.append(question_result)

    logger.debug("Total tests processed: %s", len(question_results))
    return question_results

def read_processed_chunks(source_dir: str) -> List[Dict[str, Any]]:
    processed_question_chunks: List[Dict[str, Any]] = []
    try:
        for filename in os.listdir(source_dir):
            if filename.endswith(".json"):
                file_path = os.path.join(source_dir, filename)
                with open(file_path, "r", encoding="utf-8") as f:
                    chunk = json.load(f)
                    processed_question_chunks = chunk
    except (FileNotFoundError, IOError) as e:
        logger.error(f"Error reading files: {e}")
        raise
    
    if not processed_question_chunks:
        logger.error("Processed question chunks are None or empty.")
    
    return processed_question_chunks

def save_results(test_destination_dir: str, question_results: List[TestResult], test_mode: str) -> None:
    current_datetime = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    output_file = os.path.join(test_destination_dir, f"test_output_v5_{test_mode}_{current_datetime}.csv")

    try:
        with open(output_file, "w", encoding="utf-8", newline='') as f:
            writer = csv.DictWriter(f, fieldnames=[
                "question", "enriched_question", "hit", "summary",
                "hitRelevance", "follow_up", "follow_up_on_topic", "gemini_evaluation", "gpt4o_evaluation"
            ])
            writer.writeheader()
            for result in question_results:
                writer.writerow({
                    "question": result.question,
                    "enriched_question": result.enriched_question_summary,
                    "hit": result.hit,
                    "summary": result.hit_summary,
                    "hitRelevance": result.hit_relevance,
                    "follow_up": result.follow_up,
                    "follow_up_on_topic": result.follow_up_on_topic,
                    "gemini_evaluation": result.gemini_evaluation,
                    "gpt4o_evaluation": result.gpt4o_evaluation
                })
        logger.info(f"Test results saved to: {output_file}")
    except IOError as e:
        logger.error(f"Error saving results: {e}")
        raise

def run_tests(config: ApiConfiguration, test_destination_dir: str, source_dir: str, num_questions: int = 100, questions: List[str] = None, persona_strategy: PersonaStrategy = None, llm_choice: str = '1') -> None:
    chat_client = configure_openai_for_azure(config, "chat")
    embedding_client = configure_openai_for_azure(config, "embedding")

    if not test_destination_dir:
        logger.error("Test data folder not provided")
        raise ValueError("Test destination directory not provided")

    if persona_strategy:
        if llm_choice == '1':
            questions = persona_strategy.generate_questions(chat_client, config, num_questions, logger)
        elif llm_choice == '2':
            gemini_evaluator = GeminiEvaluator()
            questions = persona_strategy.generate_questions(gemini_evaluator, config, num_questions, logger)
        else:
            logger.error("Invalid LLM choice. Exiting the test.")
            return

    if not questions:
        logger.error("Generated questions are None or empty. Exiting the test.")
        return

    test_mode = persona_strategy.__class__.__name__.replace('PersonaStrategy', '').lower()
    processed_question_chunks = read_processed_chunks(source_dir)
    question_results = process_questions(chat_client, embedding_client, config, questions, processed_question_chunks, logger, llm_choice)
    save_results(test_destination_dir, question_results, test_mode)