# Evaluating AI Learning Assistants: Persona-Based Evaluation and Cross-Model Evaluation for Content Gap Analysis

## Project Overview

This project explores the evaluation of AI learning assistants through a dual-layered approach:

1. **Persona-Based Evaluation:** Tailors AI interactions to specific user personas (e.g., Developer, Tester, Business Analyst) to assess content relevance and gap coverage.
2. **Cross-Model Evaluation:** Compares the performance of multiple language models (GPT-3.5, GPT-4, and Gemini-1.5 Pro) to identify strengths, weaknesses, and areas for improvement.

The repository includes modules for question generation, embedding similarity analysis, persona-specific evaluations, and content scoring. The evaluation pipeline is designed to deliver insights for improving learning assistants in educational and enterprise contexts.

---

## Features

- **Persona-Based Question Generation**: Generates targeted questions for Developer, Tester, and Business Analyst personas.
- **Content Gap Analysis**: Identifies missing or underdeveloped areas in AI responses based on embedding similarity and manual evaluation.
- **Cross-Model Performance Comparison**: Evaluates multiple LLMs on dimensions such as relevance, coherence, and contextual understanding.
- **Enriched Summary Evaluation**: Uses Gemini's evaluation capabilities to assess and score generated content.
- **Comprehensive Reporting and Visualizations**: Detailed metrics and graphs for comparative analysis.

---

## Folder Structure

```plaintext
├── PersonaStrategy.py         # Defines persona-specific question generation logic.
├── GeminiEvaluator.py         # Evaluates content relevance using the Gemini model.
├── TestRunner.py              # Main script for running tests and generating results.
├── DataTest.py                # Contains core logic for similarity analysis and result evaluation.
├── ApiConfiguration.py        # Configures API access for Azure OpenAI and Gemini models.
├── common_functions.py        # Utility functions for directory management and embedding generation.
├── generateVectorEmbeddings.py # Generates vector embeddings for input data.
├── outputviz.py               # Generates visualizations for results analysis.
├── input_data/                # Input files for analysis.
├── test_output/               # Output files, including test results and logs.
└── README.md                  # Documentation for the repository.
```

---

## Requirements

- **Python**: 3.8+
- **Dependencies**:
  - `openai`
  - `google.generativeai`
  - `numpy`
  - `tenacity`
  - `pandas`
  - `plotly`

Install dependencies using:

```bash
pip install -r requirements.txt
```

---

## Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/your-repo.git
   cd your-repo
   ```
2. Set environment variables:
   - `AZURE_OPENAI_API_KEY`
   - `AZURE_OPENAI_ENDPOINT`
   - `GEMINI_API_KEY`
3. Verify the `ApiConfiguration.py` settings:
   - Ensure deployment names and API keys are correctly configured.

---

## Usage

### 1. Generate Questions and Evaluate Responses

Run the `TestRunner.py` script:

```bash
python TestRunner.py
```

Select from the following options:

- Static Questions
- Developer Persona
- Tester Persona
- Business Analyst Persona

### 2. Generate Vector Embeddings

Run `generateVectorEmbeddings.py` to process input files:

```bash
python generateVectorEmbeddings.py
```

### 3. Visualize Results

Run `outputviz.py` to generate visual insights:

```bash
python outputviz.py
```

---

## Key Components

### Persona Strategies

The `PersonaStrategy` module defines tailored prompts for:

- Developers
- Testers
- Business Analysts

### Embedding Similarity

`DataTest.py` evaluates question and response quality using cosine similarity on embeddings generated by Azure OpenAI.

### Gemini Evaluation

The `GeminiEvaluator.py` module scores responses for relevance, coherence, and completeness.

---

## Visualization

### 1. Unique Summaries Queried

Grouped bar chart of unique summaries queried across models and personas.

### 2. Unique Questions Generated

Bar chart comparing the count of unique questions per persona and model.

### 3. Gemini Score Distribution

Histogram of evaluation scores by Gemini.

### 4. Hit Relevance vs Gemini Score

Scatter plot correlating hit relevance with Gemini evaluation scores.
